{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rucode.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhfaV8D3UFV-"
      },
      "source": [
        "import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTz5gzVJs954"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import roc_auc_score"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCiKUPwmUJ2V"
      },
      "source": [
        "unzip and load csv files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZAkmXU4uCGQ"
      },
      "source": [
        "path_to_zip = '/content/drive/My Drive/data/public_data.zip'"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kx4Q9FHuGip"
      },
      "source": [
        "with zipfile.ZipFile(path_to_zip) as myzip:\n",
        "    myzip.extractall()"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZ9hBManvOQ_"
      },
      "source": [
        "path_to_csv = '/content/train.csv'"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDXvYTjWvEKq"
      },
      "source": [
        "train_df = pd.read_csv(path_to_csv, sep='\\t')"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfoQ2gitV8J2"
      },
      "source": [
        "calculate number of each classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xs9c5ez1xPTJ",
        "outputId": "662c8ca5-2e4e-4c85-a570-c5ad4137b1f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print('true: ', sum(train_df.iloc[:, 1] == 'true'), 'false: ',sum(train_df.iloc[:, 1] != 'true'))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "true:  19919 false:  3964\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rC5OgL1hWI47"
      },
      "source": [
        "create new balanced dataset from old"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-F6E_va9_Riv"
      },
      "source": [
        "train_df_0 = train_df.loc[train_df.iloc[:, 1] == 'true', :].sample(3964)\n",
        "train_df_1 = train_df.loc[train_df.iloc[:, 1] == 'fake', :]\n",
        "train_balanced = pd.concat([train_df_0, train_df_1]).iloc[:, :]\n",
        "train_df, valid_df = train_test_split(train_balanced, test_size=0.2, random_state=42)\n"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qnt6fh5fWoXR"
      },
      "source": [
        "train/valid splitting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tzy3z50pzKam"
      },
      "source": [
        "x_train, x_test = train_test_split(train_df, test_size=0.2, random_state = 1)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZQ5bqXGXQNh"
      },
      "source": [
        "load multilingual bert from https://github.com/UKPLab/sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTwTzXpR6Rtn",
        "outputId": "58a94fe7-da6c-4f5a-bef6-2f4ae1490b70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        }
      },
      "source": [
        "!pip install -U sentence-transformers"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: sentence-transformers in /usr/local/lib/python3.6/dist-packages (0.3.6)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (0.22.2.post1)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: torch>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.6.0+cu101)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: nltk in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Requirement already satisfied, skipping upgrade: transformers<3.2.0,>=3.1.0 in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sentence-transformers) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.2.0->sentence-transformers) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from nltk->sentence-transformers) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.6/dist-packages (from transformers<3.2.0,>=3.1.0->sentence-transformers) (3.0.12)\n",
            "Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers<3.2.0,>=3.1.0->sentence-transformers) (2019.12.20)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from transformers<3.2.0,>=3.1.0->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers<3.2.0,>=3.1.0->sentence-transformers) (0.1.91)\n",
            "Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.6/dist-packages (from transformers<3.2.0,>=3.1.0->sentence-transformers) (20.4)\n",
            "Requirement already satisfied, skipping upgrade: tokenizers==0.8.1.rc2 in /usr/local/lib/python3.6/dist-packages (from transformers<3.2.0,>=3.1.0->sentence-transformers) (0.8.1rc2)\n",
            "Requirement already satisfied, skipping upgrade: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers<3.2.0,>=3.1.0->sentence-transformers) (0.0.43)\n",
            "Requirement already satisfied, skipping upgrade: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers<3.2.0,>=3.1.0->sentence-transformers) (0.7)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<3.2.0,>=3.1.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<3.2.0,>=3.1.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<3.2.0,>=3.1.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<3.2.0,>=3.1.0->sentence-transformers) (2020.6.20)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers<3.2.0,>=3.1.0->sentence-transformers) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers<3.2.0,>=3.1.0->sentence-transformers) (7.1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmvYUAI86xuh"
      },
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "embedder = SentenceTransformer('distiluse-base-multilingual-cased')"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOpWAvLEYT8N"
      },
      "source": [
        "drop rows with nan "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6V5OzPTs_9zR"
      },
      "source": [
        "train_df = train_df.dropna()\n",
        "valid_df = valid_df.dropna()"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kjuZdh4XtTX"
      },
      "source": [
        "define dataset and dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nrAeIAS9C2m"
      },
      "source": [
        "class Text_Dataset(object):\n",
        "    \"\"\"An abstract class representing a Dataset.\n",
        "    All other datasets should subclass it. All subclasses should override\n",
        "    ``__len__``, that provides the size of the dataset, and ``__getitem__``,\n",
        "    supporting integer indexing in range from 0 to len(self) exclusive.\n",
        "    \"\"\"\n",
        "    def __init__(self, df):\n",
        "      self.data = df\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "      text = self.data.iloc[index, 0]\n",
        "      label = int(self.data.iloc[index, 1] == 'true')\n",
        "      return text, label\n",
        "\n",
        "    def __len__(self):\n",
        "      return self.data.shape[0]"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kr_qV13E_pDi"
      },
      "source": [
        "train_dataset = Text_Dataset(train_df)\n",
        "test_dataset = Text_Dataset(valid_df)\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=True)"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsoYLTxPYjpU"
      },
      "source": [
        "define model bert + 2 layer softmax classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXbk2u4CAfOR"
      },
      "source": [
        "class bert_classifier(torch.nn.Module):\n",
        "    def __init__(self, D_in, H, D_out, embedder):\n",
        "        super(bert_classifier, self).__init__()\n",
        "        self.embedder = embedder\n",
        "        self.linear1 = torch.nn.Linear(in_features=D_in, out_features = H)\n",
        "        self.linear2 = torch.nn.Linear(in_features=H, out_features=D_out)\n",
        "\n",
        "    def forward(self, x):\n",
        "        emb = torch.Tensor(self.embedder.encode(x))\n",
        "        h_relu = F.relu(self.linear1(emb))\n",
        "        y_pred = self.linear2(h_relu)\n",
        "        return y_pred"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPjnrn5YaIfu"
      },
      "source": [
        "initialise classifier and optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLmfzGdjAsKb"
      },
      "source": [
        "N, D_in, H, D_out = 16, 512, 256, 2\n",
        "\n",
        "# Construct our model by instantiating the class defined above\n",
        "classifier = bert_classifier(D_in, H, D_out, embedder)\n",
        "classifier = classifier.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam([\n",
        "                {'params': classifier.linear1.parameters()},\n",
        "                {'params': classifier.linear2.parameters()},\n",
        "                {'params': list(classifier.embedder.parameters()), 'lr': 5e-5}], lr=1e-3)\n",
        "n_epoch = 16\n",
        "device = 'cpu'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXTYfpMwaXtM"
      },
      "source": [
        "trainig"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "houIMNGHjcae",
        "outputId": "685a27f3-94cf-4156-8073-c50010d77287",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        }
      },
      "source": [
        "loss_hist = []\n",
        "acc_hist = []\n",
        "\n",
        "for _epoch in range(n_epoch):\n",
        "    classifier.train()\n",
        "    loss_accum = 0\n",
        "    acc_accum = 0\n",
        "    for _i_step, batch in enumerate(train_loader):\n",
        "        x = batch[0]\n",
        "        y = torch.tensor(batch[1])\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        predictions = classifier(x)\n",
        "        loss = nn.functional.cross_entropy(predictions.cpu(), y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_accum += loss.item()\n",
        "        _, preds = torch.max(predictions, 1)\n",
        "        acc = sum(y == preds).double() / y.shape[0]\n",
        "        acc_accum += acc.item()\n",
        "    acc_accum_mean = acc_accum / (_i_step + 1)\n",
        "    loss_accum_mean = loss_accum / (_i_step + 1)\n",
        "    loss_hist.append(loss_accum_mean)\n",
        "    acc_hist.append(acc_accum_mean)\n",
        "\n",
        "    print('Epoch: ', _epoch+1) \n",
        "    print('Loss: ', loss_accum_mean, 'acc: ', acc_accum_mean)\n"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:  1\n",
            "Loss:  0.018103461047389337 acc:  0.9988979848866498\n",
            "Epoch:  2\n",
            "Loss:  0.01251998608728099 acc:  1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YSiTv1kadrB"
      },
      "source": [
        "save model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UlRt9DGiFjl"
      },
      "source": [
        "torch.save(classifier, '/content/drive/My Drive/classifier_final.pth')"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nv_8b4q1af7-"
      },
      "source": [
        "eval model on valid data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vpDIA-1gkQf",
        "outputId": "0fe2c660-6f94-41c8-8fa1-7780c5642934",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "    classifier.eval()\n",
        "    preds_all = torch.Tensor().long().to(device)\n",
        "    labels_all = torch.Tensor().long().to(device)\n",
        "    with torch.no_grad():\n",
        "        for _i_step, batch in enumerate(test_loader):\n",
        "            x = batch[0]\n",
        "            y = torch.tensor(batch[1])\n",
        "\n",
        "            probs = classifier(x)\n",
        "            _, preds = torch.max(probs, 1)\n",
        "\n",
        "            labels_all = torch.cat((labels_all, y), 0)\n",
        "            preds_all = torch.cat((preds_all, preds.long()), 0)\n",
        "\n",
        "    acc = sum(labels_all == preds_all).double() / preds_all.shape[0]\n"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  import sys\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9zPS0V7tXQ9",
        "outputId": "ae592037-efb8-494b-a92f-0e1922682f4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "f1_score(labels_all.numpy(), preds_all.numpy(), average='macro')"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9287433894462541"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paXPEVn-aol3"
      },
      "source": [
        "make prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2d_g4iel5I1A",
        "outputId": "47a1ca33-4241-4ed1-caa4-5455644788cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "path_to_csv = '/content/test.csv'\n",
        "test_df = pd.read_csv(path_to_csv, sep='\\t')\n",
        "test_score_dataset = Text_Dataset(test_df)\n",
        "test_score_dataloader = DataLoader(test_score_dataset, batch_size=16, shuffle=True)\n",
        "classifier.eval()\n",
        "preds_all = torch.Tensor().long().to(device)\n",
        "labels_all = torch.Tensor().long().to(device)\n",
        "with torch.no_grad():\n",
        "    for _i_step, batch in enumerate(test_score_dataloader):\n",
        "        x = batch[0]\n",
        "        y = torch.tensor(batch[1])\n",
        "\n",
        "        probs = classifier(x)\n",
        "        _, preds = torch.max(probs, 1)\n",
        "\n",
        "        preds_all = torch.cat((preds_all, preds.long()), 0)\n"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SR8aveppBZNW"
      },
      "source": [
        "for i in range(test_df.shape[0]):\n",
        "    if preds_all[i] == 0:\n",
        "        test_df.iloc[i, 1] = 'fake'\n",
        "    else:\n",
        "         test_df.iloc[i, 1] = 'true'"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzT3R20sCGiN"
      },
      "source": [
        "test_df.to_csv('result.csv')"
      ],
      "execution_count": 147,
      "outputs": []
    }
  ]
}